{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + \"/quantization/models\")\n",
    "from AlexNet import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(object):\n",
    "    def __init__(self, config):\n",
    "        self.train_path = config[\"train_path\"]\n",
    "        self.test_path = config[\"test_path\"]\n",
    "        self.num_epoch = config[\"num_epoch\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.input_size = config[\"input_size\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        \n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "        self.device = None\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "    \n",
    "    \n",
    "    def load_data(self):\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.CenterCrop(self.input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.CenterCrop(self.input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        train_set = ImageFolder(self.train_path, transform=train_transform)\n",
    "        test_set = ImageFolder(self.test_path, transform=test_transform)\n",
    "        \n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        \n",
    "    def show(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def load_model(self, model):\n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        print(\"train:\")\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('Epoch {}/{}'.format(epoch+1, self.num_epoch))\n",
    "            print('-'*15)\n",
    "\n",
    "            for batch_num, (data, label) in enumerate(self.train_loader):\n",
    "                data, label = data.to(self.device), label.to(self.device)\n",
    "            \n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, label)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                prediction = torch.max(output, 1)\n",
    "                total += label.size(0)\n",
    "                train_correct += np.sum(prediction[1].cpu().numpy() == label.cpu().numpy())\n",
    "                \n",
    "                print(batch_num, len(self.train_loader), 'Loss: %.4f | Acc: %.3f%% (%d/%d)'\n",
    "                        % (train_loss / (batch_num + 1), 100. * train_correct / total, train_correct, total))\n",
    "\n",
    "        return train_loss, train_correct / total\n",
    "    \n",
    "    def test(self):\n",
    "        print(\"test:\")\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (data, label) in enumerate(self.test_loader):\n",
    "                data, label = data.to(self.device), label.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, label)\n",
    "                test_loss += loss.item()\n",
    "                prediction = torch.max(output, 1)\n",
    "                total += label.size(0)\n",
    "                test_correct += np.sum(prediction[1].cpu().numpy() == label.cpu().numpy())\n",
    "\n",
    "                print(batch_num, len(self.test_loader), 'Loss: %.4f | Acc: %.3f%% (%d/%d)'\n",
    "                             % (test_loss / (batch_num + 1), 100. * test_correct / total, test_correct, total))\n",
    "\n",
    "        return test_loss, test_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    \"train_path\": \"/home/yurii.laba/Projects/research/datasets/waste-classification-data/dataset/DATASET/TRAIN\",\n",
    "    \"test_path\": \"/home/yurii.laba/Projects/research/datasets/waste-classification-data/dataset/DATASET/TEST\",\n",
    "    \"input_size\": 227,\n",
    "    \"batch_size\": 100,\n",
    "    \"num_epoch\": 1,\n",
    "    \"lr\": 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "Epoch 1/1\n",
      "---------------\n",
      "0 226 Loss: 0.6855 | Acc: 62.000% (62/100)\n",
      "1 226 Loss: 30.0310 | Acc: 61.000% (122/200)\n",
      "2 226 Loss: 20.3053 | Acc: 54.667% (164/300)\n",
      "3 226 Loss: 15.4219 | Acc: 52.750% (211/400)\n",
      "4 226 Loss: 12.4783 | Acc: 49.600% (248/500)\n",
      "5 226 Loss: 10.5159 | Acc: 47.667% (286/600)\n",
      "6 226 Loss: 9.1136 | Acc: 47.429% (332/700)\n",
      "7 226 Loss: 8.0580 | Acc: 47.875% (383/800)\n",
      "8 226 Loss: 7.2335 | Acc: 49.444% (445/900)\n",
      "9 226 Loss: 6.5856 | Acc: 49.200% (492/1000)\n",
      "10 226 Loss: 6.0458 | Acc: 49.545% (545/1100)\n",
      "11 226 Loss: 5.5971 | Acc: 50.667% (608/1200)\n",
      "12 226 Loss: 5.2184 | Acc: 50.846% (661/1300)\n",
      "13 226 Loss: 4.8903 | Acc: 51.786% (725/1400)\n",
      "14 226 Loss: 4.5993 | Acc: 53.267% (799/1500)\n",
      "15 226 Loss: 4.3496 | Acc: 54.188% (867/1600)\n",
      "16 226 Loss: 4.1272 | Acc: 55.235% (939/1700)\n",
      "17 226 Loss: 3.9300 | Acc: 56.111% (1010/1800)\n",
      "18 226 Loss: 3.7525 | Acc: 57.105% (1085/1900)\n",
      "19 226 Loss: 3.5944 | Acc: 57.750% (1155/2000)\n",
      "20 226 Loss: 3.4499 | Acc: 58.381% (1226/2100)\n",
      "21 226 Loss: 3.3198 | Acc: 58.636% (1290/2200)\n",
      "22 226 Loss: 3.1963 | Acc: 59.522% (1369/2300)\n",
      "23 226 Loss: 3.0869 | Acc: 60.083% (1442/2400)\n",
      "24 226 Loss: 2.9856 | Acc: 60.600% (1515/2500)\n",
      "25 226 Loss: 2.8949 | Acc: 60.731% (1579/2600)\n",
      "26 226 Loss: 2.8056 | Acc: 61.185% (1652/2700)\n",
      "27 226 Loss: 2.7206 | Acc: 61.786% (1730/2800)\n",
      "28 226 Loss: 2.6485 | Acc: 62.207% (1804/2900)\n",
      "29 226 Loss: 2.5882 | Acc: 62.267% (1868/3000)\n",
      "30 226 Loss: 2.5190 | Acc: 62.903% (1950/3100)\n",
      "31 226 Loss: 2.4561 | Acc: 63.406% (2029/3200)\n",
      "32 226 Loss: 2.3981 | Acc: 63.939% (2110/3300)\n",
      "33 226 Loss: 2.3453 | Acc: 64.382% (2189/3400)\n",
      "34 226 Loss: 2.2957 | Acc: 64.800% (2268/3500)\n",
      "35 226 Loss: 2.2461 | Acc: 65.028% (2341/3600)\n",
      "36 226 Loss: 2.1984 | Acc: 65.405% (2420/3700)\n",
      "37 226 Loss: 2.1533 | Acc: 65.737% (2498/3800)\n",
      "38 226 Loss: 2.1122 | Acc: 65.897% (2570/3900)\n",
      "39 226 Loss: 2.0703 | Acc: 66.250% (2650/4000)\n",
      "40 226 Loss: 2.0332 | Acc: 66.341% (2720/4100)\n",
      "41 226 Loss: 1.9968 | Acc: 66.667% (2800/4200)\n",
      "42 226 Loss: 1.9634 | Acc: 66.930% (2878/4300)\n",
      "43 226 Loss: 1.9317 | Acc: 67.068% (2951/4400)\n",
      "44 226 Loss: 1.8999 | Acc: 67.333% (3030/4500)\n",
      "45 226 Loss: 1.8711 | Acc: 67.478% (3104/4600)\n",
      "46 226 Loss: 1.8430 | Acc: 67.532% (3174/4700)\n",
      "47 226 Loss: 1.8156 | Acc: 67.771% (3253/4800)\n",
      "48 226 Loss: 1.7892 | Acc: 68.020% (3333/4900)\n",
      "49 226 Loss: 1.7634 | Acc: 68.220% (3411/5000)\n",
      "50 226 Loss: 1.7374 | Acc: 68.471% (3492/5100)\n",
      "51 226 Loss: 1.7144 | Acc: 68.558% (3565/5200)\n",
      "52 226 Loss: 1.6904 | Acc: 68.811% (3647/5300)\n",
      "53 226 Loss: 1.6672 | Acc: 68.981% (3725/5400)\n",
      "54 226 Loss: 1.6478 | Acc: 69.127% (3802/5500)\n",
      "55 226 Loss: 1.6271 | Acc: 69.357% (3884/5600)\n",
      "56 226 Loss: 1.6078 | Acc: 69.421% (3957/5700)\n",
      "57 226 Loss: 1.5877 | Acc: 69.603% (4037/5800)\n",
      "58 226 Loss: 1.5686 | Acc: 69.729% (4114/5900)\n",
      "59 226 Loss: 1.5510 | Acc: 69.933% (4196/6000)\n",
      "60 226 Loss: 1.5323 | Acc: 70.131% (4278/6100)\n",
      "61 226 Loss: 1.5150 | Acc: 70.290% (4358/6200)\n",
      "62 226 Loss: 1.4968 | Acc: 70.540% (4444/6300)\n",
      "63 226 Loss: 1.4809 | Acc: 70.703% (4525/6400)\n",
      "64 226 Loss: 1.4654 | Acc: 70.846% (4605/6500)\n",
      "65 226 Loss: 1.4503 | Acc: 70.924% (4681/6600)\n",
      "66 226 Loss: 1.4359 | Acc: 71.030% (4759/6700)\n",
      "67 226 Loss: 1.4213 | Acc: 71.176% (4840/6800)\n",
      "68 226 Loss: 1.4070 | Acc: 71.348% (4923/6900)\n",
      "69 226 Loss: 1.3934 | Acc: 71.500% (5005/7000)\n",
      "70 226 Loss: 1.3811 | Acc: 71.493% (5076/7100)\n",
      "71 226 Loss: 1.3682 | Acc: 71.611% (5156/7200)\n",
      "72 226 Loss: 1.3558 | Acc: 71.644% (5230/7300)\n",
      "73 226 Loss: 1.3441 | Acc: 71.743% (5309/7400)\n",
      "74 226 Loss: 1.3321 | Acc: 71.840% (5388/7500)\n",
      "75 226 Loss: 1.3214 | Acc: 71.816% (5458/7600)\n",
      "76 226 Loss: 1.3106 | Acc: 71.935% (5539/7700)\n",
      "77 226 Loss: 1.3001 | Acc: 72.038% (5619/7800)\n",
      "78 226 Loss: 1.2898 | Acc: 72.114% (5697/7900)\n",
      "79 226 Loss: 1.2793 | Acc: 72.263% (5781/8000)\n",
      "80 226 Loss: 1.2680 | Acc: 72.444% (5868/8100)\n",
      "81 226 Loss: 1.2583 | Acc: 72.524% (5947/8200)\n",
      "82 226 Loss: 1.2488 | Acc: 72.602% (6026/8300)\n",
      "83 226 Loss: 1.2393 | Acc: 72.702% (6107/8400)\n",
      "84 226 Loss: 1.2289 | Acc: 72.847% (6192/8500)\n",
      "85 226 Loss: 1.2184 | Acc: 73.000% (6278/8600)\n",
      "86 226 Loss: 1.2097 | Acc: 73.080% (6358/8700)\n",
      "87 226 Loss: 1.2021 | Acc: 73.114% (6434/8800)\n",
      "88 226 Loss: 1.1928 | Acc: 73.225% (6517/8900)\n",
      "89 226 Loss: 1.1838 | Acc: 73.333% (6600/9000)\n",
      "90 226 Loss: 1.1777 | Acc: 73.308% (6671/9100)\n",
      "91 226 Loss: 1.1698 | Acc: 73.380% (6751/9200)\n",
      "92 226 Loss: 1.1628 | Acc: 73.462% (6832/9300)\n",
      "93 226 Loss: 1.1546 | Acc: 73.564% (6915/9400)\n",
      "94 226 Loss: 1.1489 | Acc: 73.537% (6986/9500)\n",
      "95 226 Loss: 1.1425 | Acc: 73.542% (7060/9600)\n",
      "96 226 Loss: 1.1358 | Acc: 73.577% (7137/9700)\n",
      "97 226 Loss: 1.1283 | Acc: 73.684% (7221/9800)\n",
      "98 226 Loss: 1.1216 | Acc: 73.788% (7305/9900)\n",
      "99 226 Loss: 1.1154 | Acc: 73.850% (7385/10000)\n",
      "100 226 Loss: 1.1080 | Acc: 73.990% (7473/10100)\n",
      "101 226 Loss: 1.1013 | Acc: 74.049% (7553/10200)\n",
      "102 226 Loss: 1.0947 | Acc: 74.117% (7634/10300)\n",
      "103 226 Loss: 1.0878 | Acc: 74.212% (7718/10400)\n",
      "104 226 Loss: 1.0809 | Acc: 74.343% (7806/10500)\n",
      "105 226 Loss: 1.0750 | Acc: 74.415% (7888/10600)\n",
      "106 226 Loss: 1.0698 | Acc: 74.421% (7963/10700)\n",
      "107 226 Loss: 1.0628 | Acc: 74.556% (8052/10800)\n",
      "108 226 Loss: 1.0576 | Acc: 74.587% (8130/10900)\n",
      "109 226 Loss: 1.0524 | Acc: 74.627% (8209/11000)\n",
      "110 226 Loss: 1.0470 | Acc: 74.676% (8289/11100)\n",
      "111 226 Loss: 1.0408 | Acc: 74.777% (8375/11200)\n",
      "112 226 Loss: 1.0357 | Acc: 74.805% (8453/11300)\n",
      "113 226 Loss: 1.0305 | Acc: 74.842% (8532/11400)\n",
      "114 226 Loss: 1.0253 | Acc: 74.887% (8612/11500)\n",
      "115 226 Loss: 1.0213 | Acc: 74.888% (8687/11600)\n",
      "116 226 Loss: 1.0160 | Acc: 74.983% (8773/11700)\n",
      "117 226 Loss: 1.0105 | Acc: 75.068% (8858/11800)\n",
      "118 226 Loss: 1.0053 | Acc: 75.160% (8944/11900)\n",
      "119 226 Loss: 0.9999 | Acc: 75.242% (9029/12000)\n",
      "120 226 Loss: 0.9958 | Acc: 75.264% (9107/12100)\n",
      "121 226 Loss: 0.9900 | Acc: 75.369% (9195/12200)\n",
      "122 226 Loss: 0.9849 | Acc: 75.431% (9278/12300)\n",
      "123 226 Loss: 0.9804 | Acc: 75.468% (9358/12400)\n",
      "124 226 Loss: 0.9753 | Acc: 75.536% (9442/12500)\n",
      "125 226 Loss: 0.9709 | Acc: 75.595% (9525/12600)\n",
      "126 226 Loss: 0.9661 | Acc: 75.669% (9610/12700)\n",
      "127 226 Loss: 0.9613 | Acc: 75.758% (9697/12800)\n",
      "128 226 Loss: 0.9563 | Acc: 75.806% (9779/12900)\n",
      "129 226 Loss: 0.9522 | Acc: 75.846% (9860/13000)\n",
      "130 226 Loss: 0.9478 | Acc: 75.947% (9949/13100)\n",
      "131 226 Loss: 0.9438 | Acc: 76.000% (10032/13200)\n",
      "132 226 Loss: 0.9392 | Acc: 76.098% (10121/13300)\n",
      "133 226 Loss: 0.9351 | Acc: 76.164% (10206/13400)\n",
      "134 226 Loss: 0.9313 | Acc: 76.222% (10290/13500)\n",
      "135 226 Loss: 0.9280 | Acc: 76.228% (10367/13600)\n",
      "136 226 Loss: 0.9249 | Acc: 76.277% (10450/13700)\n",
      "137 226 Loss: 0.9210 | Acc: 76.319% (10532/13800)\n",
      "138 226 Loss: 0.9181 | Acc: 76.309% (10607/13900)\n",
      "139 226 Loss: 0.9146 | Acc: 76.364% (10691/14000)\n",
      "140 226 Loss: 0.9108 | Acc: 76.404% (10773/14100)\n",
      "141 226 Loss: 0.9074 | Acc: 76.465% (10858/14200)\n",
      "142 226 Loss: 0.9041 | Acc: 76.503% (10940/14300)\n",
      "143 226 Loss: 0.9012 | Acc: 76.486% (11014/14400)\n",
      "144 226 Loss: 0.8981 | Acc: 76.524% (11096/14500)\n",
      "145 226 Loss: 0.8948 | Acc: 76.534% (11174/14600)\n",
      "146 226 Loss: 0.8910 | Acc: 76.592% (11259/14700)\n",
      "147 226 Loss: 0.8877 | Acc: 76.608% (11338/14800)\n",
      "148 226 Loss: 0.8861 | Acc: 76.591% (11412/14900)\n",
      "149 226 Loss: 0.8836 | Acc: 76.620% (11493/15000)\n",
      "150 226 Loss: 0.8800 | Acc: 76.675% (11578/15100)\n",
      "151 226 Loss: 0.8771 | Acc: 76.704% (11659/15200)\n",
      "152 226 Loss: 0.8747 | Acc: 76.706% (11736/15300)\n",
      "153 226 Loss: 0.8717 | Acc: 76.740% (11818/15400)\n",
      "154 226 Loss: 0.8691 | Acc: 76.768% (11899/15500)\n",
      "155 226 Loss: 0.8660 | Acc: 76.833% (11986/15600)\n",
      "156 226 Loss: 0.8629 | Acc: 76.879% (12070/15700)\n",
      "157 226 Loss: 0.8600 | Acc: 76.930% (12155/15800)\n",
      "158 226 Loss: 0.8570 | Acc: 76.975% (12239/15900)\n",
      "159 226 Loss: 0.8547 | Acc: 76.987% (12318/16000)\n",
      "160 226 Loss: 0.8526 | Acc: 76.988% (12395/16100)\n",
      "161 226 Loss: 0.8505 | Acc: 76.975% (12470/16200)\n",
      "162 226 Loss: 0.8487 | Acc: 76.982% (12548/16300)\n",
      "163 226 Loss: 0.8462 | Acc: 76.976% (12624/16400)\n",
      "164 226 Loss: 0.8439 | Acc: 76.970% (12700/16500)\n",
      "165 226 Loss: 0.8414 | Acc: 77.018% (12785/16600)\n",
      "166 226 Loss: 0.8391 | Acc: 77.048% (12867/16700)\n",
      "167 226 Loss: 0.8363 | Acc: 77.107% (12954/16800)\n",
      "168 226 Loss: 0.8337 | Acc: 77.112% (13032/16900)\n",
      "169 226 Loss: 0.8309 | Acc: 77.159% (13117/17000)\n",
      "170 226 Loss: 0.8281 | Acc: 77.216% (13204/17100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 226 Loss: 0.8257 | Acc: 77.233% (13284/17200)\n",
      "172 226 Loss: 0.8231 | Acc: 77.283% (13370/17300)\n",
      "173 226 Loss: 0.8205 | Acc: 77.333% (13456/17400)\n",
      "174 226 Loss: 0.8186 | Acc: 77.366% (13539/17500)\n",
      "175 226 Loss: 0.8170 | Acc: 77.386% (13620/17600)\n",
      "176 226 Loss: 0.8146 | Acc: 77.435% (13706/17700)\n",
      "177 226 Loss: 0.8127 | Acc: 77.438% (13784/17800)\n",
      "178 226 Loss: 0.8101 | Acc: 77.492% (13871/17900)\n",
      "179 226 Loss: 0.8082 | Acc: 77.500% (13950/18000)\n",
      "180 226 Loss: 0.8064 | Acc: 77.514% (14030/18100)\n",
      "181 226 Loss: 0.8047 | Acc: 77.527% (14110/18200)\n",
      "182 226 Loss: 0.8027 | Acc: 77.552% (14192/18300)\n",
      "183 226 Loss: 0.8004 | Acc: 77.587% (14276/18400)\n",
      "184 226 Loss: 0.7982 | Acc: 77.627% (14361/18500)\n",
      "185 226 Loss: 0.7961 | Acc: 77.651% (14443/18600)\n",
      "186 226 Loss: 0.7949 | Acc: 77.636% (14518/18700)\n",
      "187 226 Loss: 0.7928 | Acc: 77.665% (14601/18800)\n",
      "188 226 Loss: 0.7911 | Acc: 77.693% (14684/18900)\n",
      "189 226 Loss: 0.7894 | Acc: 77.721% (14767/19000)\n",
      "190 226 Loss: 0.7871 | Acc: 77.775% (14855/19100)\n",
      "191 226 Loss: 0.7850 | Acc: 77.802% (14938/19200)\n",
      "192 226 Loss: 0.7829 | Acc: 77.834% (15022/19300)\n",
      "193 226 Loss: 0.7810 | Acc: 77.866% (15106/19400)\n",
      "194 226 Loss: 0.7793 | Acc: 77.867% (15184/19500)\n",
      "195 226 Loss: 0.7772 | Acc: 77.903% (15269/19600)\n",
      "196 226 Loss: 0.7750 | Acc: 77.949% (15356/19700)\n",
      "197 226 Loss: 0.7728 | Acc: 77.995% (15443/19800)\n",
      "198 226 Loss: 0.7716 | Acc: 77.980% (15518/19900)\n",
      "199 226 Loss: 0.7700 | Acc: 77.985% (15597/20000)\n",
      "200 226 Loss: 0.7682 | Acc: 77.995% (15677/20100)\n",
      "201 226 Loss: 0.7662 | Acc: 78.035% (15763/20200)\n",
      "202 226 Loss: 0.7642 | Acc: 78.059% (15846/20300)\n",
      "203 226 Loss: 0.7624 | Acc: 78.083% (15929/20400)\n",
      "204 226 Loss: 0.7605 | Acc: 78.098% (16010/20500)\n",
      "205 226 Loss: 0.7583 | Acc: 78.136% (16096/20600)\n",
      "206 226 Loss: 0.7566 | Acc: 78.164% (16180/20700)\n",
      "207 226 Loss: 0.7552 | Acc: 78.183% (16262/20800)\n",
      "208 226 Loss: 0.7535 | Acc: 78.215% (16347/20900)\n",
      "209 226 Loss: 0.7521 | Acc: 78.238% (16430/21000)\n",
      "210 226 Loss: 0.7504 | Acc: 78.256% (16512/21100)\n",
      "211 226 Loss: 0.7489 | Acc: 78.283% (16596/21200)\n",
      "212 226 Loss: 0.7472 | Acc: 78.310% (16680/21300)\n",
      "213 226 Loss: 0.7454 | Acc: 78.336% (16764/21400)\n",
      "214 226 Loss: 0.7437 | Acc: 78.377% (16851/21500)\n",
      "215 226 Loss: 0.7422 | Acc: 78.398% (16934/21600)\n",
      "216 226 Loss: 0.7410 | Acc: 78.396% (17012/21700)\n",
      "217 226 Loss: 0.7392 | Acc: 78.431% (17098/21800)\n",
      "218 226 Loss: 0.7374 | Acc: 78.466% (17184/21900)\n",
      "219 226 Loss: 0.7361 | Acc: 78.477% (17265/22000)\n",
      "220 226 Loss: 0.7346 | Acc: 78.498% (17348/22100)\n",
      "221 226 Loss: 0.7329 | Acc: 78.532% (17434/22200)\n",
      "222 226 Loss: 0.7313 | Acc: 78.556% (17518/22300)\n",
      "223 226 Loss: 0.7297 | Acc: 78.589% (17604/22400)\n",
      "224 226 Loss: 0.7281 | Acc: 78.609% (17687/22500)\n",
      "225 226 Loss: 0.7270 | Acc: 78.612% (17738/22564)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(164.3119662106037, 0.7861194823612835)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = AlexNet(2)\n",
    "pipeline = Pipeline(pipeline_config)\n",
    "pipeline.load_data()\n",
    "pipeline.load_model(alexnet)\n",
    "pipeline.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      "0 26 Loss: 0.1334 | Acc: 94.000% (94/100)\n",
      "1 26 Loss: 0.1483 | Acc: 95.000% (190/200)\n",
      "2 26 Loss: 0.1369 | Acc: 96.333% (289/300)\n",
      "3 26 Loss: 0.1211 | Acc: 97.000% (388/400)\n",
      "4 26 Loss: 0.1199 | Acc: 97.200% (486/500)\n",
      "5 26 Loss: 0.1280 | Acc: 96.167% (577/600)\n",
      "6 26 Loss: 0.1283 | Acc: 96.143% (673/700)\n",
      "7 26 Loss: 0.1267 | Acc: 96.500% (772/800)\n",
      "8 26 Loss: 0.1333 | Acc: 96.333% (867/900)\n",
      "9 26 Loss: 0.1386 | Acc: 95.800% (958/1000)\n",
      "10 26 Loss: 0.1345 | Acc: 96.000% (1056/1100)\n",
      "11 26 Loss: 0.1355 | Acc: 96.083% (1153/1200)\n",
      "12 26 Loss: 0.1412 | Acc: 95.846% (1246/1300)\n",
      "13 26 Loss: 0.1460 | Acc: 95.429% (1336/1400)\n",
      "14 26 Loss: 0.1500 | Acc: 95.467% (1432/1500)\n",
      "15 26 Loss: 0.1545 | Acc: 95.562% (1529/1600)\n",
      "16 26 Loss: 0.1637 | Acc: 95.412% (1622/1700)\n",
      "17 26 Loss: 0.1728 | Acc: 95.056% (1711/1800)\n",
      "18 26 Loss: 0.2222 | Acc: 92.579% (1759/1900)\n",
      "19 26 Loss: 0.2550 | Acc: 90.850% (1817/2000)\n",
      "20 26 Loss: 0.2888 | Acc: 89.714% (1884/2100)\n",
      "21 26 Loss: 0.3081 | Acc: 89.000% (1958/2200)\n",
      "22 26 Loss: 0.3156 | Acc: 88.870% (2044/2300)\n",
      "23 26 Loss: 0.3248 | Acc: 88.542% (2125/2400)\n",
      "24 26 Loss: 0.3339 | Acc: 88.240% (2206/2500)\n",
      "25 26 Loss: 0.3423 | Acc: 88.221% (2217/2513)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.898750744760036, 0.8822124950258655)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
